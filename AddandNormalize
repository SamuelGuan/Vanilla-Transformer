import torch.nn as nn
class AddandNormalize(nn.Module):
    #残差连接与归一化模块
    #使用的是LayerNormalize,参数肯定是句子长度
    def __init__(self,sentence_len):
        super(AddandNormalize, self).__init__()
        self.norm = nn.LayerNorm(sentence_len)

    def forward(self,residual,MHAorFFN_result):
        return self.norm((residual + MHAorFFN_result).transpose(-1,-2)).transpose(-1,-2)
