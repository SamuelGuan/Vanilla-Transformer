import numpy as np
import torch
import math
import torch.nn as nn

class PositionalEncoding(nn.Module):
    def __init__(self, d_model, sentence_len):
        #d_model是词向量维度,max_len是句子最大长度，dropout是神经元失效比率，或者叫置0率
        super(PositionalEncoding, self).__init__()

        pe = torch.zeros(sentence_len, d_model)
        sin_position = torch.arange(0, sentence_len, 2).unsqueeze(1)
        cos_position = torch.arange(1, sentence_len, 2).unsqueeze(1)
        div_term = torch.exp(
            torch.arange(0, d_model) * -(math.log(10000.0) / d_model)
        )
        pe[0::2, :] = torch.sin(sin_position * div_term)
        pe[1::2, :] = torch.cos(cos_position * div_term)
        #unsqueeze函数用于增加的方括号保证数组的维度一致
        self.pe = pe
        print(pe.shape)

    def forward(self, x):
        x = x + self.pe[:x.size(0), : x.size(1)].requires_grad_(False)
        return x
