import torch.nn as nn

class FeedForward(nn.Module):
    #d_hidden是FFN的隐藏层大小,有论文论证过只需要比输入向量元素个数多1个即可
    def __init__(self,d_model,bias=False):
        super(FeedForward, self).__init__()
        d_hidden = d_model + 5
        self.fc = nn.Sequential(
            nn.Linear(d_model, d_hidden, bias=bias),
            nn.ReLU(),
            nn.Linear(d_hidden, d_model, bias=bias))

    def forward(self,AN_result):
        return self.fc(AN_result)
